{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using United States server backend.\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "import re\n",
    "import emoji\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import  Pool\n",
    "import time\n",
    "import translators as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove emojis in text, since these conflict during translation\n",
    "def remove_emoji(text):\n",
    "    return emoji.get_emoji_regexp().sub(u'', text)\n",
    "\n",
    "\n",
    "def approximate_emoji_insert(string, index,char):\n",
    "    if(index<(len(string)-1)):\n",
    "        \n",
    "        while(string[index]!=' ' ):\n",
    "            if(index+1==len(string)):\n",
    "                break\n",
    "            index=index+1\n",
    "        return string[:index] + ' '+char + ' ' + string[index:]\n",
    "    else:\n",
    "        return string + ' '+char + ' ' \n",
    "    \n",
    "\n",
    "\n",
    "def extract_emojis(str1):\n",
    "    try:\n",
    "        return [(c,i) for i,c in enumerate(str1) if c in emoji.UNICODE_EMOJI]\n",
    "    except AttributeError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('translatedtrain'):\n",
    "    os.makedirs('translatedtrain')\n",
    "if not os.path.exists('translatedval'):\n",
    "    os.makedirs('translatedval')\n",
    "if not os.path.exists('translatedtest'):\n",
    "    os.makedirs('translatedtest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use multiprocessing framework for speeding up translation process\n",
    "def parallelize_dataframe(df, func, n_cores=1):\n",
    "    '''parallelize the dataframe'''\n",
    "    # df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    # df = pd.concat(pool.map(func, df))\n",
    "    # pool.close()\n",
    "    # pool.join()\n",
    "    # print(\"done parallelizing\")\n",
    "    return func(df)\n",
    "\n",
    "# Main function for translation\n",
    "def translate(x,lang):\n",
    "    '''provide the translation given text and the language'''\n",
    "    #x=preprocess_lib.preprocess_multi(x,lang,multiple_sentences=False,stop_word_remove=False, tokenize_word=False, tokenize_sentence=False)\n",
    "    # translator = Translator()\n",
    "    # translator = Translator(to_lang='en',from_lang='pt')\n",
    "    emoji_list=extract_emojis(x)\n",
    "    # print(\"try\")\n",
    "    # translated_text = translator.translate(x, src = 'pt', dest='en')\n",
    "    # translated_text = translated_text.text\n",
    "    try:\n",
    "        # print(\"try\")\n",
    "        # translated_text = translator.translate(x)\n",
    "        # translated_text=translate_text(x,'pt','en')\n",
    "        translated_text= ts.google(x, from_language='id', to_language='en')\n",
    "        # print(\"translated successfully\")\n",
    "    except:\n",
    "        translated_text=x\n",
    "        print(\"not translated\")\n",
    "    for ele in emoji_list:\n",
    "        translated_text=approximate_emoji_insert(translated_text, ele[1],ele[0])\n",
    "    # print(\"done translating\")\n",
    "    return translated_text\n",
    "\n",
    "def add_features(df):\n",
    "    '''adding new features to the dataframe'''\n",
    "    translated_text=[]\n",
    "    for index,row in df.iterrows():\n",
    "        # if('pt' in ['en','unk']):\n",
    "        #     translated_text.append(row['text'])\n",
    "        # else:\n",
    "        translated_text.append(translate(row['text'], 'id'))    \n",
    "    df[\"translated\"]=translated_text\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "train_files = glob.glob('/Dataset/train/*csv')\n",
    "test_files = glob.glob('test/*.csv')\n",
    "val_files = glob.glob('val/*.csv')\n",
    "files= train_files+test_files+val_files\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "size=10\n",
    "\n",
    "for file in files:\n",
    "    wp_data=pd.read_csv(file)\n",
    "    end= len(wp_data.index)\n",
    "    list_df=[]\n",
    "    for i in tqdm_notebook(range(0,end,size)):\n",
    "                print(i,\"_iteration\")\n",
    "                df_new=parallelize_dataframe(wp_data[i:i+size],add_features,n_cores=1)\n",
    "                list_df.append(df_new)\n",
    "    df_translated=pd.concat(list_df,ignore_index=True)\n",
    "    file_name='translated'+file\n",
    "    df_translated.to_csv(file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import translators as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'word' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5850be4d84fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoogle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_language\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'word' is not defined"
     ]
    }
   ],
   "source": [
    "ts.google(word, from_language='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('hate': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1cef7c7ee159f2ff4f8f1db1ffec408e54b31c52d6d2f9b440b7e2f236dc03d1"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}